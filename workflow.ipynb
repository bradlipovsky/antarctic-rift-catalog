{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make an Antarctic Rift Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Read the ATL06 files into a Python data structure\n",
    "Put the needed info in a dictionary, save the whole thing to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on the mask...\n",
      "     Loading existing ckdt.\n",
      "     Mask loaded after 6.980409 s\n",
      "ERROR: Key error, ATL06_20190407203152_01460310_004_01.h5\n",
      "ERROR: Key error, ATL06_20190407203152_01460310_004_01.h5\n",
      "ERROR: Key error, ATL06_20190407203152_01460310_004_01.h5\n",
      "ERROR: Key error, ATL06_20190407203152_01460310_004_01.h5\n",
      "ERROR: Key error, ATL06_20190527053933_09000312_004_01.h5\n",
      "ERROR: Key error, ATL06_20190527053933_09000312_004_01.h5\n",
      "ERROR: Key error, ATL06_20190527053933_09000312_004_01.h5\n",
      "ERROR: Key error, ATL06_20190527053933_09000312_004_01.h5\n",
      "ERROR: Key error, ATL06_20200618123724_12820712_004_01.h5\n",
      "ERROR: Key error, ATL06_20200618123724_12820712_004_01.h5\n",
      "ERROR: Key error, ATL06_20200810084911_07020812_004_01.h5\n",
      "ERROR: Key error, ATL06_20200810084911_07020812_004_01.h5\n",
      "ERROR: Key error, ATL06_20200810084911_07020812_004_01.h5\n",
      "ERROR: Key error, ATL06_20200810084911_07020812_004_01.h5\n",
      "ERROR: File not found,  ATL06_20210203112931_06331010_004_01.h5\n",
      "Time to read the H5 files:  18.532987341983244\n",
      "Time to apply ice shelf mask:  188.6199249290512\n",
      "Time to save all of the data:  0.6407995769986883\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import arc\n",
    "import importlib\n",
    "importlib.reload(arc)\n",
    "import json\n",
    "\n",
    "shelf_name='brunt-riiser-ekstrom'\n",
    "\n",
    "atl06_file_name = './atl06_' + shelf_name + '-atl06.pkl'\n",
    "atl06_filelist = './filelists/' + shelf_name + '-list.json'\n",
    "dataset_path = '/data/fast0/'\n",
    "\n",
    "if os.path.isfile(atl06_file_name):\n",
    "    print(\"Data already saved, so there's no need to ingest data. \\\n",
    "To repeat the data ingest, it would probably be best to change the filename of the \\\n",
    "existing file.\")\n",
    "else:\n",
    "    with open(atl06_filelist,'rb') as handle:\n",
    "        filelist = json.load(handle)\n",
    "    arc.ingest(filelist,atl06_file_name,dataset_path)\n",
    "\n",
    "'''\n",
    "ATLAS was on safehold from June 25 to July 25, 2019.  Did we download data from this time?\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.  Run the rift detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATL06 data loaded from pickle in 0.343149 s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Pool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-9fd6569b90cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pyTMD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'datasets/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnproc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mwith_tides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pool' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from functools import partial\n",
    "importlib.reload(arc)\n",
    "tt = time.perf_counter()\n",
    "\n",
    "# Load data (deserialize)\n",
    "with open(atl06_file_name, 'rb') as handle:\n",
    "    output = pickle.load(handle)\n",
    "    print('ATL06 data loaded from pickle in %f s'%(time.perf_counter()-tt))\n",
    "\n",
    "\n",
    "# with_tides = output.apply (lambda row: arc.run_pyTMD(row,dataset_path+'datasets/'), axis=1)\n",
    "from multiprocessing import Pool\n",
    "func = partial(arc.run_pyTMD, dataset_path+'datasets/')\n",
    "nproc = 8\n",
    "with Pool(nproc) as p:\n",
    "    with_tides = p.map(func, output)\n",
    "\n",
    "\n",
    "# Find the rifts\n",
    "# rift_obs = arc.get_rifts(atl06_data)\n",
    "\n",
    "# Store the rifts in a dataframe\n",
    "# rift_obs=pd.DataFrame(rift_obs)\n",
    "\n",
    "# Or you could store it in a geopanda\n",
    "# import geopandas\n",
    "# rift_obs = geopandas.GeoDataFrame(rift_obs,\n",
    "#                              geometry=geopandas.points_from_xy(rift_obs['x-centroid'],\n",
    "#                                                                rift_obs['y-centroid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Step 3. Save the rift_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rift_obs_output_file_name = shelf_name + '_rift_obs.pickle'\n",
    "with open(rift_obs_output_file_name, 'wb') as handle:\n",
    "    pickle.dump(rift_obs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Steps.\n",
    "\n",
    "From this point, useful next steps are to quality control the data (notebook qc.ipynb) and to analyze the data (analyze_rift_measurements.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
